{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNimDgO628cYEh0Wbfa/fVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shriju/RAG-Langchain1/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFEaBi-5RdsG",
        "outputId": "4aa08ea9-59dd-49ec-eb51-6e15ec7ba8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.21)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.18)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.15)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.137)\n",
            "Collecting numpy<2,>=1 (from langchain_community)\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.32.0.20241016)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.4)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.67.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.10)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.28.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.1)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.49b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Installing collected packages: protobuf, numpy\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 protobuf-5.28.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "# # pip install 'protobuf<=3.20.1' --force-reinstall\n",
        "# # !python3 -m pip install pip --upgrade\n",
        "# # !pip install pyopenssl --upgrade\n",
        "# !pip install pymupdf\n",
        "# !pip install langchain-cohere"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-metadata --upgrade\n",
        "!pip install tf-keras --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zt_3RV8t1sea",
        "outputId": "c38d063a-85f3-452e-e085-f09af16e2c1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Requirement already satisfied: absl-py<3.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata) (2.1.0)\n",
            "Collecting protobuf<4.21,>=3.20.3 (from tensorflow-metadata)\n",
            "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.28.3\n",
            "    Uninstalling protobuf-5.28.3:\n",
            "      Successfully uninstalled protobuf-5.28.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.28.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "10d8755a5b6a4a7aa468fdc11f23778d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tf-keras\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
            "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-keras\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "Successfully installed tf-keras-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kSnoQ0s2IOT",
        "outputId": "9f64ef47-e001-447b-a55b-2477ed5cd88d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Collecting tensorflow==2.17.0\n",
            "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (75.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.67.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.0)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
            "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.17.1 tensorflow-2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras==2.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RMa_LTZ2xKK",
        "outputId": "07f3e426-71de-4e7e-8981-5b2af18553fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-keras==2.17.0\n",
            "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tf-keras==2.17.0) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (75.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.6.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras==2.17.0) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras==2.17.0) (0.1.2)\n",
            "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-keras\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.18.0\n",
            "    Uninstalling tf_keras-2.18.0:\n",
            "      Successfully uninstalled tf_keras-2.18.0\n",
            "Successfully installed tf-keras-2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install 'protobuf<=3.20.1' --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "hgGV1q2rYVze",
        "outputId": "12bcc5fe-c6c1-4e55-e9c0-5825cbc826d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf<=3.20.1\n",
            "  Using cached protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
            "Using cached protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "opentelemetry-proto 1.28.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "6ba8b8cfd63f4e19a673dfa9e9210e4e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install pip --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcxwCuAeYrzb",
        "outputId": "2842c427-a03d-4f48-8b0e-b529707b4045"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X3kjJfgY85x",
        "outputId": "b84609ef-df7c-4a44-953d-6e68bb73c1d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyopenssl --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQg93mwG3Wjh",
        "outputId": "e72f5733-a016-4f08-8e60-3dc9eaeb617d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyopenssl in /usr/local/lib/python3.10/dist-packages (24.2.1)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyopenssl) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=41.0.5->pyopenssl) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyopenssl) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ty2OXCY4V6",
        "outputId": "4320aadb-6461-4b29-ce5c-f1c561f4b532"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.24.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyopenssl --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyCVdu6hY0yv",
        "outputId": "3f6a81ff-f367-4513-a7e9-4eaaa1ad7052"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyopenssl in /usr/local/lib/python3.10/dist-packages (24.2.1)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyopenssl) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=41.0.5->pyopenssl) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyopenssl) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "# from langchain_community.llms import Ollama\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "t2ajuHmJYGj3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RhV91fh6QdP",
        "outputId": "5776f1d8-fa1e-4c3f-98be-14074586ebc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=fe86436fce63ff361a793a11819c4a8054a86e537a9b4e6699f24412c8199678\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader, ArxivLoader"
      ],
      "metadata": {
        "id": "PMtsw0dv5Bkh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = ''\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
        "\n",
        "# os.environ['COHERE_API_KEY'] = \"\"\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "qSl2hAUlZmmF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Paid\n",
        "llm= ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "embeddings=OpenAIEmbeddings()\n",
        "\n",
        "### Free\n",
        "# llm = Ollama(model=\"llama3\")\n",
        "# embeddings=CohereEmbeddings()"
      ],
      "metadata": {
        "id": "EaMfr79lu7kc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d8pQM3f1vZSY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Load the documents\n",
        "docs = ArxivLoader(query='2312.10997', load_max_docs=1, load_all_available_meta=True).load()\n",
        "load_docs = docs[0].page_content\n",
        "\n"
      ],
      "metadata": {
        "id": "Jj45Ny-95pqc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Filter complex metadata\n",
        "filtered_docs = filter_complex_metadata(docs)\n",
        "\n",
        "# Step 3: Split the documents\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(filtered_docs)\n",
        "\n",
        "# Step 4: Embed the documents\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Step 5: Create the retriever\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "r0KHyHlH5s_V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "rag_chain.invoke(\"What is Modular RAG ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UJwarmlhwMTn",
        "outputId": "b8e0ea58-b56a-480e-af20-84ed9752aa22"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Modular RAG is an architecture that advances beyond previous RAG paradigms by offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. The shift towards a modular RAG approach supports both sequential processing and integrated end-to-end training across its components.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to write prompts- templates"
      ],
      "metadata": {
        "id": "MNxS4MS__Ikk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"You are a Q&A assistant, You will refer the context provided and answer the question.\n",
        "If you dont know the answer , reply that you dont know the answer:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbWQY_TZ-_Jt",
        "outputId": "af8d5246-be2c-4671-bae0-dc58691d445d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='You are a Q&A assistant, You will refer the context provided and answer the question. \\nIf you dont know the answer , reply that you dont know the answer:\\n{context}\\nQuestion: {question}\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiquery Perspective"
      ],
      "metadata": {
        "id": "DoPlAiaj_uJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant.\n",
        "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector\n",
        "database.\n",
        "By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question:\n",
        "{question}\"\"\"\n",
        "\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is Modular RAG ?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKnYGEIg_Qqj",
        "outputId": "7c5f135e-3aeb-4278-f194-ca23ef668372"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "databases. This comprehensive review paper offers a detailed\n",
            "examination of the progression of RAG paradigms, encompassing\n",
            "the Naive RAG, the Advanced RAG, and the Modular RAG.\n",
            "It meticulously scrutinizes the tripartite foundation of RAG\n",
            "frameworks, which includes the retrieval, the generation and the\n",
            "augmentation techniques. The paper highlights the state-of-the-\n",
            "art technologies embedded in each of these critical components,\n",
            "providing a profound understanding of the advancements in RAG\n",
            "systems. Furthermore, this paper introduces up-to-date evalua-\n",
            "tion framework and benchmark. At the end, this article delineates\n",
            "the challenges currently faced and points out prospective avenues\n",
            "for research and development 1.\n",
            "Index Terms—Large language model, retrieval-augmented gen-\n",
            "eration, natural language processing, information retrieval\n",
            "I. INTRODUCTION\n",
            "L\n",
            "ARGE language models (LLMs) have achieved remark-\n",
            "able success, though they still face significant limitations,\n",
            "especially in domain-specific or knowledge-intensive tasks [1],\n",
            "notably producing “hallucinations” [2] when handling queries\n",
            "beyond their training data or requiring current information. To\n",
            "overcome challenges, Retrieval-Augmented Generation (RAG)\n",
            "enhances LLMs by retrieving relevant document chunks from\n",
            "external knowledge base through semantic similarity calcu-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "from operator import itemgetter\n",
        "\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "vrl4KrVMTYxf",
        "outputId": "923c0c57-decc-4e69-b9fd-6964c58d1bc7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Modular RAG is an advanced approach that goes beyond the traditional Naive and Advanced RAG paradigms by offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Modular RAG introduces additional specialized components to enhance retrieval and processing capabilities, allowing for module substitution or reconfiguration to address specific challenges. This approach enhances the applicability of RAG across different tasks by integrating new modules or adjusting interaction flow among existing ones.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decomposition"
      ],
      "metadata": {
        "id": "0RYr1aIjUZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "question = \"I dont understand RAG ,Can you help me understand what are the components and one more thing I would like to know about whether is it same as Advanced RAG ?\"\n",
        "#### I gave an ambigious query which talks about 3 questions 1. RAG understanding 2. Components of RAG 3. Difference between RAG & Advanced RAG\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
        "\n",
        "questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "subZ25vsTpqB",
        "outputId": "eb51a7de-a756-47e0-aad0-9eb0d041f076"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. What are the components of RAG analysis?',\n",
              " '2. How does RAG analysis differ from Advanced RAG analysis?',\n",
              " '3. Can you provide a detailed explanation of RAG analysis for better understanding?']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions:\n",
        "\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
        "\n",
        "print(q_a_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diND-yhUUxgo",
        "outputId": "9ea94fea-3f1b-4d9d-a333-ada94e19f49b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Question: 1. What are the components of RAG analysis?\n",
            "Answer: The components of RAG analysis include retrieval, generation, and augmentation techniques. These components work together to enhance the accuracy and credibility of the generation process, particularly for knowledge-intensive tasks.\n",
            "---\n",
            "Question: 2. How does RAG analysis differ from Advanced RAG analysis?\n",
            "Answer: RAG analysis differs from Advanced RAG analysis in terms of specific improvements and enhancements made to overcome limitations. Advanced RAG focuses on enhancing retrieval quality by employing pre-retrieval and post-retrieval strategies, refining indexing techniques through approaches like a sliding window, fine-grained segmentation, and metadata incorporation. It also incorporates optimization methods to streamline the retrieval process. In contrast, RAG analysis represents the earlier methodology, with Advanced RAG being a response to the limitations observed in the Naive RAG approach.\n",
            "---\n",
            "Question: 3. Can you provide a detailed explanation of RAG analysis for better understanding?\n",
            "Answer: RAG analysis, which stands for Retrieval-Augmented Generation analysis, is a methodology that enhances the accuracy and credibility of the generation process, particularly for knowledge-intensive tasks. It involves three main components: retrieval, generation, and augmentation techniques.\n",
            "\n",
            "1. **Retrieval**: This component focuses on retrieving relevant information from external databases to supplement the knowledge of the language model being used. By incorporating external knowledge, the generation process can be enriched with up-to-date and domain-specific information.\n",
            "\n",
            "2. **Generation**: The generation component involves the actual process of generating text or responses based on the retrieved information. This step utilizes the retrieved knowledge to enhance the quality and relevance of the generated content.\n",
            "\n",
            "3. **Augmentation**: The augmentation techniques aim to further refine and optimize the generated output. This may involve post-generation processes, fine-tuning of language models, and incorporating additional context or information to improve the overall quality of the generated content.\n",
            "\n",
            "Overall, RAG analysis synergistically combines the intrinsic knowledge of large language models with external databases to create more accurate and credible outputs. It represents a comprehensive approach to leveraging external knowledge sources to enhance the capabilities of language models for a wide range of tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It answered all the questions one by one Now its time to look at the actual query and revert it in a single answer.Lets print it and see what happens !"
      ],
      "metadata": {
        "id": "m1TXAdvCVsyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV4xj8d3Vmly",
        "outputId": "04736605-a942-4dd8-c8c3-23bdc628acc2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG analysis, which stands for Retrieval-Augmented Generation analysis, is a methodology that enhances the accuracy and credibility of the generation process, particularly for knowledge-intensive tasks. It involves three main components: retrieval, generation, and augmentation techniques.\n",
            "\n",
            "1. **Retrieval**: This component focuses on retrieving relevant information from external databases to supplement the knowledge of the language model being used. By incorporating external knowledge, the generation process can be enriched with up-to-date and domain-specific information.\n",
            "\n",
            "2. **Generation**: The generation component involves the actual process of generating text or responses based on the retrieved information. This step utilizes the retrieved knowledge to enhance the quality and relevance of the generated content.\n",
            "\n",
            "3. **Augmentation**: The augmentation techniques aim to further refine and optimize the generated output. This may involve post-generation processes, fine-tuning of language models, and incorporating additional context or information to improve the overall quality of the generated content.\n",
            "\n",
            "Overall, RAG analysis synergistically combines the intrinsic knowledge of large language models with external databases to create more accurate and credible outputs. It represents a comprehensive approach to leveraging external knowledge sources to enhance the capabilities of language models for a wide range of tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYDE: Hypothecation document"
      ],
      "metadata": {
        "id": "q22-Oy-9dgPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# HyDE document genration\n",
        "template = \"\"\"Please write a scientific paper passage to answer the question\n",
        "Question: {question}\n",
        "Passage:\"\"\"\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "generate_docs_for_retrieval = (\n",
        "    prompt_hyde | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run\n",
        "generate_docs_for_retrieval.invoke({\"question\":question})\n",
        "# Retrieve\n",
        "retrieval_chain = generate_docs_for_retrieval | retriever\n",
        "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
        "retireved_docs\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "uWrWRruidj42",
        "outputId": "9c8d722b-4c53-4c45-cbc6-177d74d5dd52"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by incorporating knowledge from external databases. The components of RAG include retrieval, generation, and augmentation techniques. \\n\\n- Retrieval: This component focuses on sourcing context from external databases to provide additional information for the generation process.\\n- Generation: The generation component synthesizes coherent and relevant answers based on the retrieved context.\\n- Augmentation: This component integrates the retrieved information with the task at hand, ensuring relevance and consistency in the generated responses.\\n\\nAdvanced RAG is an improved version of the Naive RAG, which introduces specific enhancements to overcome limitations. It focuses on enhancing retrieval quality by employing pre-retrieval and post-retrieval strategies, refining indexing techniques, and incorporating optimization methods. Advanced RAG represents a progression in the development of RAG frameworks, offering more sophisticated approaches to knowledge integration and generation processes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Fusion"
      ],
      "metadata": {
        "id": "L-iZsV3Aq-eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion\n",
        "template = \"\"\"You are an assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\n",
        "\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "question = \"What is pattern in Modular RAG ?\"\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jxtQnlQVzdP",
        "outputId": "f6d53a91-c7b2-4b15-f7b5-3f95060afae6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='comprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings'), 0.04972677595628415), (Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent'), 0.04945355191256831), (Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='Hybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].'), 0.03225806451612903), (Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='enhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by'), 0.031746031746031744), (Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='The summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.\\n17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep'), 0.016129032258064516), (Document(metadata={'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Published': '2024-03-27', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'comment': 'Ongoing Work', 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'primary_category': 'cs.CL', 'published_first_time': '2023-12-18'}, page_content='its indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].\\n4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common'), 0.015873015873015872)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"\n",
        "Answer the following question based on this context,\n",
        "If you dont find any answer then just revert with 'Answer not found'.\n",
        "context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "#llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "T4xhlpY7skui",
        "outputId": "091bcab5-b61d-468d-ac08-7227ab6dda2d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pattern in Modular RAG refers to the adaptability and versatility offered by allowing module substitution or reconfiguration to address specific challenges. It goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Splitting Strategies"
      ],
      "metadata": {
        "id": "0JR_ELNMtnWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Character Text Splitting\n",
        "print(\"#### Character Text Splitting ####\")\n",
        "\n",
        "text = \"\"\"In 2024, India will find itself at the center of global attention, hosting both the highly anticipated general elections and the Cricket World Cup. On the political front, the general elections will see over 900 million eligible voters making their voices heard in a democratic exercise unparalleled in scale.\n",
        "Political parties are already mobilizing their bases, with campaigns focused on critical issues like economic growth, social justice, and national security. Meanwhile, cricket fever will grip the nation as teams from around the world compete for glory in the ICC Cricket World Cup.\n",
        "Stadia will roar with the cheers of passionate fans, and cricket pitches will become the stage for thrilling displays of skill and sportsmanship.\n",
        "As politicians rally for votes and cricketers battle for the championship, these parallel events will underscore the dual fervor that defines India's national identity: a deep commitment to democracy and an unbridled love for cricket\n",
        "\"\"\"\n",
        "# Manual Splitting\n",
        "chunks = []\n",
        "chunk_size = 35 # Characters\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
        "print(documents)\n",
        "\n",
        "# Automatic Text Splitting\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
        "documents = text_splitter.create_documents([text])\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBkHvHRetoID",
        "outputId": "ddc4da25-da1a-4f02-e0fc-71d8ae7cb6c3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Character Text Splitting ####\n",
            "[Document(metadata={'source': 'local'}, page_content='In 2024, India will find itself at '), Document(metadata={'source': 'local'}, page_content='the center of global attention, hos'), Document(metadata={'source': 'local'}, page_content='ting both the highly anticipated ge'), Document(metadata={'source': 'local'}, page_content='neral elections and the Cricket Wor'), Document(metadata={'source': 'local'}, page_content='ld Cup. On the political front, the'), Document(metadata={'source': 'local'}, page_content=' general elections will see over 90'), Document(metadata={'source': 'local'}, page_content='0 million eligible voters making th'), Document(metadata={'source': 'local'}, page_content='eir voices heard in a democratic ex'), Document(metadata={'source': 'local'}, page_content='ercise unparalleled in scale. \\nPoli'), Document(metadata={'source': 'local'}, page_content='tical parties are already mobilizin'), Document(metadata={'source': 'local'}, page_content='g their bases, with campaigns focus'), Document(metadata={'source': 'local'}, page_content='ed on critical issues like economic'), Document(metadata={'source': 'local'}, page_content=' growth, social justice, and nation'), Document(metadata={'source': 'local'}, page_content='al security. Meanwhile, cricket fev'), Document(metadata={'source': 'local'}, page_content='er will grip the nation as teams fr'), Document(metadata={'source': 'local'}, page_content='om around the world compete for glo'), Document(metadata={'source': 'local'}, page_content='ry in the ICC Cricket World Cup. \\nS'), Document(metadata={'source': 'local'}, page_content='tadia will roar with the cheers of '), Document(metadata={'source': 'local'}, page_content='passionate fans, and cricket pitche'), Document(metadata={'source': 'local'}, page_content='s will become the stage for thrilli'), Document(metadata={'source': 'local'}, page_content='ng displays of skill and sportsmans'), Document(metadata={'source': 'local'}, page_content='hip. \\nAs politicians rally for vote'), Document(metadata={'source': 'local'}, page_content='s and cricketers battle for the cha'), Document(metadata={'source': 'local'}, page_content='mpionship, these parallel events wi'), Document(metadata={'source': 'local'}, page_content='ll underscore the dual fervor that '), Document(metadata={'source': 'local'}, page_content=\"defines India's national identity: \"), Document(metadata={'source': 'local'}, page_content='a deep commitment to democracy and '), Document(metadata={'source': 'local'}, page_content='an unbridled love for cricket\\n')]\n",
            "[Document(metadata={}, page_content='In 2024, India will find itself at '), Document(metadata={}, page_content='the center of global attention, hos'), Document(metadata={}, page_content='ting both the highly anticipated ge'), Document(metadata={}, page_content='neral elections and the Cricket Wor'), Document(metadata={}, page_content='ld Cup. On the political front, the'), Document(metadata={}, page_content=' general elections will see over 90'), Document(metadata={}, page_content='0 million eligible voters making th'), Document(metadata={}, page_content='eir voices heard in a democratic ex'), Document(metadata={}, page_content='ercise unparalleled in scale. \\nPoli'), Document(metadata={}, page_content='tical parties are already mobilizin'), Document(metadata={}, page_content='g their bases, with campaigns focus'), Document(metadata={}, page_content='ed on critical issues like economic'), Document(metadata={}, page_content=' growth, social justice, and nation'), Document(metadata={}, page_content='al security. Meanwhile, cricket fev'), Document(metadata={}, page_content='er will grip the nation as teams fr'), Document(metadata={}, page_content='om around the world compete for glo'), Document(metadata={}, page_content='ry in the ICC Cricket World Cup. \\nS'), Document(metadata={}, page_content='tadia will roar with the cheers of '), Document(metadata={}, page_content='passionate fans, and cricket pitche'), Document(metadata={}, page_content='s will become the stage for thrilli'), Document(metadata={}, page_content='ng displays of skill and sportsmans'), Document(metadata={}, page_content='hip. \\nAs politicians rally for vote'), Document(metadata={}, page_content='s and cricketers battle for the cha'), Document(metadata={}, page_content='mpionship, these parallel events wi'), Document(metadata={}, page_content='ll underscore the dual fervor that '), Document(metadata={}, page_content=\"defines India's national identity: \"), Document(metadata={}, page_content='a deep commitment to democracy and '), Document(metadata={}, page_content='an unbridled love for cricket\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Recursive Character Text Splitting\n",
        "print(\"#### Recursive Character Text Splitting ####\")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=15) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
        "print(text_splitter.create_documents([text]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbYkvoU9uVSD",
        "outputId": "2141a6b0-583a-4748-cc1a-b69942955039"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Recursive Character Text Splitting ####\n",
            "[Document(metadata={}, page_content='In 2024, India will find itself at the center of global'), Document(metadata={}, page_content='of global attention, hosting both the highly anticipated general'), Document(metadata={}, page_content='general elections and the Cricket World Cup. On the political'), Document(metadata={}, page_content='the political front, the general elections will see over 900'), Document(metadata={}, page_content='see over 900 million eligible voters making their voices heard'), Document(metadata={}, page_content='voices heard in a democratic exercise unparalleled in scale.'), Document(metadata={}, page_content='Political parties are already mobilizing their bases, with'), Document(metadata={}, page_content='bases, with campaigns focused on critical issues like economic'), Document(metadata={}, page_content='like economic growth, social justice, and national security.'), Document(metadata={}, page_content='security. Meanwhile, cricket fever will grip the nation as teams'), Document(metadata={}, page_content='as teams from around the world compete for glory in the ICC'), Document(metadata={}, page_content='in the ICC Cricket World Cup.'), Document(metadata={}, page_content='Stadia will roar with the cheers of passionate fans, and cricket'), Document(metadata={}, page_content='and cricket pitches will become the stage for thrilling displays'), Document(metadata={}, page_content='displays of skill and sportsmanship.'), Document(metadata={}, page_content='As politicians rally for votes and cricketers battle for the'), Document(metadata={}, page_content='battle for the championship, these parallel events will'), Document(metadata={}, page_content=\"events will underscore the dual fervor that defines India's\"), Document(metadata={}, page_content=\"India's national identity: a deep commitment to democracy and an\"), Document(metadata={}, page_content='and an unbridled love for cricket')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Document Specific Splitting\n",
        "print(\"#### Document Specific Splitting ####\")\n",
        "\n",
        "# Document Specific Splitting - Markdown\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
        "markdown_text = text\n",
        "print(splitter.create_documents([markdown_text]))\n",
        "\n",
        "# Document Specific Splitting - Python\n",
        "from langchain.text_splitter import PythonCodeTextSplitter\n",
        "python_text = \"\"\"\n",
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "\n",
        "for i in range(10):\n",
        "    print (i)\n",
        "\"\"\"\n",
        "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "print(python_splitter.create_documents([python_text]))\n",
        "\n",
        "# Document Specific Splitting - Javascript\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "javascript_text = \"\"\"\n",
        "// Function is called, the return value will end up in x\n",
        "let x = myFunction(4, 3);\n",
        "\n",
        "function myFunction(a, b) {\n",
        "  // Function returns the product of a and b\n",
        "  return a * b;\n",
        "}\n",
        "\"\"\"\n",
        "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
        ")\n",
        "print(js_splitter.create_documents([javascript_text]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg1TbH1pu2-3",
        "outputId": "ff2d9fc0-b48f-4c48-91b4-346539d3919c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Document Specific Splitting ####\n",
            "[Document(metadata={}, page_content='In 2024, India will find itself at the'), Document(metadata={}, page_content='center of global attention, hosting'), Document(metadata={}, page_content='both the highly anticipated general'), Document(metadata={}, page_content='elections and the Cricket World Cup. On'), Document(metadata={}, page_content='the political front, the general'), Document(metadata={}, page_content='elections will see over 900 million'), Document(metadata={}, page_content='eligible voters making their voices'), Document(metadata={}, page_content='heard in a democratic exercise'), Document(metadata={}, page_content='unparalleled in scale.'), Document(metadata={}, page_content='Political parties are already'), Document(metadata={}, page_content='mobilizing their bases, with campaigns'), Document(metadata={}, page_content='focused on critical issues like'), Document(metadata={}, page_content='economic growth, social justice, and'), Document(metadata={}, page_content='national security. Meanwhile, cricket'), Document(metadata={}, page_content='fever will grip the nation as teams'), Document(metadata={}, page_content='from around the world compete for glory'), Document(metadata={}, page_content='in the ICC Cricket World Cup.'), Document(metadata={}, page_content='Stadia will roar with the cheers of'), Document(metadata={}, page_content='passionate fans, and cricket pitches'), Document(metadata={}, page_content='will become the stage for thrilling'), Document(metadata={}, page_content='displays of skill and sportsmanship.'), Document(metadata={}, page_content='As politicians rally for votes and'), Document(metadata={}, page_content='cricketers battle for the championship,'), Document(metadata={}, page_content='these parallel events will underscore'), Document(metadata={}, page_content=\"the dual fervor that defines India's\"), Document(metadata={}, page_content='national identity: a deep commitment to'), Document(metadata={}, page_content='democracy and an unbridled love for'), Document(metadata={}, page_content='cricket')]\n",
            "[Document(metadata={}, page_content='class Person:\\n  def __init__(self, name, age):\\n    self.name = name\\n    self.age = age'), Document(metadata={}, page_content='p1 = Person(\"John\", 36)\\n\\nfor i in range(10):\\n    print (i)')]\n",
            "[Document(metadata={}, page_content='// Function is called, the return value will end up in x'), Document(metadata={}, page_content='let x = myFunction(4, 3);'), Document(metadata={}, page_content='function myFunction(a, b) {'), Document(metadata={}, page_content='// Function returns the product of a and b\\n  return a * b;\\n}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Semantic Chunking\n",
        "print(\"#### Semantic Chunking ####\")\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Percentile - all differences between sentences are calculated, and then any difference greater than the X percentile is split\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
        "text_splitter = SemanticChunker(\n",
        "    OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
        ")\n",
        "documents = text_splitter.create_documents([text])\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co37Vc2PvGqv",
        "outputId": "fbc08626-467c-4f0f-e274-85bccac0f979"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Semantic Chunking ####\n",
            "[Document(metadata={}, page_content='In 2024, India will find itself at the center of global attention, hosting both the highly anticipated general elections and the Cricket World Cup. On the political front, the general elections will see over 900 million eligible voters making their voices heard in a democratic exercise unparalleled in scale. Political parties are already mobilizing their bases, with campaigns focused on critical issues like economic growth, social justice, and national security. Meanwhile, cricket fever will grip the nation as teams from around the world compete for glory in the ICC Cricket World Cup.'), Document(metadata={}, page_content=\"Stadia will roar with the cheers of passionate fans, and cricket pitches will become the stage for thrilling displays of skill and sportsmanship. As politicians rally for votes and cricketers battle for the championship, these parallel events will underscore the dual fervor that defines India's national identity: a deep commitment to democracy and an unbridled love for cricket\\n\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FrPkTbiPUYj5"
      }
    }
  ]
}